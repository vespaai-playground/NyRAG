# LLM API configuration (required for chat answers)
LLM_BASE_URL=https://openrouter.ai/api/v1
LLM_API_KEY=your-llm-api-key-here
LLM_MODEL=openai/gpt-5.1

# Vespa backend selection
# NYRAG_LOCAL=1  # local via VespaDocker (default if no Vespa Cloud env is set)
# NYRAG_LOCAL=0  # Vespa Cloud via VespaCloud

# Vespa Cloud (required when NYRAG_LOCAL=0)
# VESPA_CLOUD_TENANT=your-tenant
# VESPA_CLOUD_APPLICATION=your-app  # optional; defaults to generated app name (e.g. nyrag<projectname>)
# VESPA_CLOUD_INSTANCE=default
# VESPA_CLOUD_API_KEY_PATH=/path/to/api-key.pem
# VESPA_CLOUD_API_KEY="-----BEGIN PRIVATE KEY-----..."

# Bring-your-own Vespa (skip deploy)
# NYRAG_VESPA_DEPLOY=0
# VESPA_URL=http://localhost
# VESPA_PORT=8080  # optional; defaults to 8080 for local, 443 for cloud

# Optional mTLS for Vespa endpoints (required for Vespa Cloud dataplane)
# VESPA_CLIENT_CERT=/path/to/data-plane-public-cert.pem
# VESPA_CLIENT_KEY=/path/to/data-plane-private-key.pem
# VESPA_CA_CERT=/path/to/ca.pem
# VESPA_TLS_VERIFY=1

# Python settings
PYTHONUNBUFFERED=1
