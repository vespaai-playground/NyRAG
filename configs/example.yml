name: webrag
mode: web
start_loc: https://vespa.ai/
exclude:
  - https://vespa.ai/pricing
  - https://vespa.ai/sales
  - https://status.vespa.ai/*

# Optional: Web crawling specific parameters
crawl_params:
  respect_robots_txt: true     # Respect robots.txt rules (default: true)
  aggressive_crawl: false       # Enable aggressive crawling (higher speed, more requests) (default: false)
  follow_subdomains: true       # Follow subdomains of the start URL (default: true)
  strict_mode: false            # Only crawl URLs matching the start URL pattern (default: false)
  user_agent_type: chrome       # User agent type: chrome, firefox, safari, mobile, bot (default: chrome)
  # custom_user_agent: "..."    # Custom user agent string (optional, overrides user_agent_type)
  allowed_domains:            # Explicitly allowed domains (optional, auto-detected from start_loc)
    - vespa.ai
    - docs.vespa.ai

# Optional: General parameters for downstream RAG processing
rag_params:
  embedding_model: sentence-transformers/all-MiniLM-L6-v2
  embedding_dim: 384              # Embedding dimension (default: 384)
  chunk_size: 1024                # Chunk size for text splitting (default: 1024)
  chunk_overlap: 50
  distance_metric: angular        # Distance metric: angular, euclidean, etc. (default: angular)
  max_tokens: 8192
  
  # Optional: LLM configuration (works with any OpenAI-compatible API)
  # If not set, will use environment variables (LLM_BASE_URL, LLM_MODEL, LLM_API_KEY)
  # or fall back to OpenRouter (OPENROUTER_API_KEY, OPENROUTER_MODEL)
  
  # For OpenRouter (default):
  # llm_base_url: https://openrouter.ai/api/v1
  # llm_model: anthropic/claude-3.5-sonnet
  # llm_api_key: your-openrouter-key
  
  # For Ollama (local):
  # llm_base_url: http://localhost:11434/v1
  # llm_model: llama3.2
  # llm_api_key: dummy
  
  # For LM Studio (local):
  # llm_base_url: http://localhost:1234/v1
  # llm_model: openai/gpt-oss-20b
  # llm_api_key: dummy
  

  # For vLLM (local or remote):
  # llm_base_url: http://localhost:8000/v1
  # llm_model: meta-llama/Llama-3.2-3B-Instruct
  # llm_api_key: dummy
